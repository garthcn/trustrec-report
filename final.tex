
\documentclass[12pt]{article}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage{subfig}
\geometry{letterpaper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

% See the ``Article customise'' template for come common customisations

\title{Final Report}
\author{}
%\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Recommender System is becoming a crucial part of our online experience these days. It is widely used in various websites to recommend all kinds of products to customers: movies\footnote[1]{www.flixster.com}, books\footnote[2]{www.amazon.com}, music\footnote[3]{www.pandora.com}, etc. The quality of a recommender system is not only related to user experience, but also revenues of online business.  

Different recommendation methods have been proposed in both industry and academia. The performance of recommender systems is usually measured with two metrics: accuracy and coverage. Higher accuracy means the estimated rating of one item for a given user is more likely to match the user's own rating of that item; higher coverage means the system is able to predict more items' ratings for more users. 

One of the most popular recommendation method is Collaborative Filtering (CF). CF compares rating profiles of all items and identify \emph{similar items} using Pearson Correlation. When asked to predict the rating of a target item for a target user, CF first finds items which are similar to the target item, then it returns the target user's average ratings of these similar items (weighted by their similarities to the target item). Though CF has been adopted by many recommender systems, it performs poorly for cold-start users (i.e. new users of a website). Since cold-start users have rated only a few items, it is often difficult for CF to find similar items rated by the target user. Therefore, coverage of CF is usually very low for cold-start users. 

To overcome the weakness of CF on cold-start users, Mohsen Jamali and Martin Ester proposed Trustwalker (citation here). Trustwalker combines trust-based method and CF by performing a random walk on the target user's neighbours (both direct and indirect). Trustwalker managed to achieve lower Root Mean Square Error (RMSE) and higher coverage compared to traditional CF. 

To explore potential improvement in these recommender systems mentioned above, we propose a new recommendation method, which utilizes users' expertise of reviewing products from different categories. We first calculate all users' expertise based on the number and qualities of their reviews, then we pick users with the highest expertise and call them \emph{experts}. Lastly, we use experts' ratings to predict the ratings of other users. 

The rest of the report is organized as follows: section 2 introduces the methodology of our experiments; section 3 reports the dataset we crawled from Epinions.com and our experiment results on that dataset; section 4 concludes the project. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
Methodology.

\subsection{Identifying Experts} % (fold)
\label{sub:identifying_experts}

Given a category $C$ and a set of users $U = \{{u_{1}, ..., u_{n}}\}$ who have written at least one review in category $C$, we primarily consider two factors in calculating a user's expertise: (1) the average quality of the user's reviews, and (2) the total number of user's reviews. We use the following formulas to calculate a user's expertise in a category $C$:
\begin{equation}
E(u) = \frac{\sum_{i=1}^n R_{u, i}} {n} \times f(n)
\end{equation}
\begin{equation}
f(n) = \frac{1}{1 + e^{\frac{-10n} {MAX\_REVIEWS(C)}}}
\end{equation}
where $R_{u, i}$ is the rating for user $u$'s review $i$ in category $C$ and $f(n)$ is a sigmoid function to avoid favoring the number of reviews too much. It is based on the assumption that a user written 100 reviews has more expertise than a user written 10 while a user written written 1000 reviews is almost as good as a user written 900 reviews. $MAX\_REVIEWS(C)$ is the maximum number of reviews per user in category $C$. $f(n)$ is tailored for every category, i.e., the user who has written the most reviews in a category have $f(n)=1$.

% subsection identifying_experts (end)

\subsection{Top $k$ Experts Recommendation} % (fold)
\label{sub:expert_recommendation}

Previous research has demonstrated that recommending items to users based on \emph{expert} opinions can be as good as traditional user-based \emph{Collaborative Filtering} \cite{Amatriain:2009p101}. We propose a similar expert recommendation algorithm: given an item $i$ that belongs to a category $C$, the recommender first picks out the top $k$ percent experts in category $C$; then for each expert, if the expert has rated item $i$, the exact rating is returned; otherwise a weighted average of similar items of $i$ is returned; finally the recommender averages the ratings of all selected experts by their expertise as the predicted rating for item $i$. Item similarity is calculated using \emph{Pearson Correlation}:
\begin{equation}
	corr(i,j) = \frac{\sum_{u \in U_{i,j}} (r_{u,i} - \bar{r}_u)(r_{u,j} - \bar{r}_u)} {{\sqrt{\sum_{u \in U_{i,j}} (r_{u,i} - \bar{r}_u)}} \sqrt{\sum_{u \in U_{i,j}} (r_{u,j} - \bar{r}_u)}}
\end{equation}
where $U_{i,j}$ is the set of common users who have rated both items $i$ and $j$, and $\bar{r}_u$ denotes the average of ratings expressed by $u$.

% subsection expert_recommendation (end)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment}
Experiment.
\subsection{Dataset}
We tested different recommender systems on a comprehensive dataset from Epinions.com, which is a product reviewing site for consumers. Its product catalogue covers a large number of product categories, including Music, Books, Movies, Automobiles, Electronics, etc. For each product, consumers provide ratings on a scale from 1 to 5 (5 being the best), write detailed reviews on products and express the helpfulness of existing reviews. Besides, users can express directed trust among each other: if user A trusts user B, user B does not have to trust user A. In other words, there is a implicit directed trust graph among all users. 

We crawled a comprehensive dataset from Epinions.com, which includes user info, product reviews, review helpfulness, trust links, etc. The size of the crawled dataset is shown in Table ~\ref{data_size}. Among all the 91,584 users, about 1000 users have written more than 100 reviews. These top 1000 most productive users have contributed to 55.9\% reviews (338,173 out of 605,066).  Table ~\ref{rating_distro} shows the distribution of ratings among all products. One third of the ratings are 5, and about 60\% of the ratings are 4 or 5. The distribution is very different from another popular dataset -- MovieLens, whose rating distribution is more diverse (citation here). Table ~\ref{helpful_distro} shows the helpfulnesses of all reviews. Similarly, about 75\% of all reviews are considered \emph{very helpful}. 

Table ~\ref{category_distro} shows the number of reviews in the top 5 categories. The top 5 categories account for more than half of all reviews on Epinions.com.

\begin{table}
	\centering
	\subfloat[Dataset Size\label{data_size}] {
		\begin{tabular}[b] {| l | r | }
			\hline
			Data Type & Size \\ \hline \hline
			Users & 91,584 \\ \hline
			Reviews & 605,066 \\ \hline
			Trust Links & 1,152,095 \\
			\hline
		\end{tabular}
	}	
	\subfloat[Rating Distribution\label{rating_distro}] {	
		\begin{tabular}[b] {| c | r |}
		\hline
		Rating & Percentage \\ \hline \hline
		5 & 33\% \\ \hline
		4 & 27\% \\ \hline
		3 & 20\% \\ \hline
		2 & 13\% \\ \hline
		1 & 7\% \\
		\hline
		\end{tabular}
	}
	\subfloat[Review Helpfulness Distribution \label{helpful_distro}] {		
		\begin{tabular}[b] {| l | r | }
		\hline
		Review Helpfulness & Percentage \\ \hline \hline
		Very Helpful & 74.3\% \\ \hline
		Helpful & 15.4\% \\ \hline
		Somewhat Helpful & 6.8\% \\ \hline
		Show & 2.4\% \\ \hline
		Not Yet Rated & 1\% \\
		\hline
		\end{tabular}
	}

	\caption{Dataset Summary}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular} {| l | c | r | }
		\hline
		Category & Number of Reviews & Percentage \\ \hline \hline
		Movies & 102,461 & 16.9\% \\ \hline
		Books & 72,940 & 12.1\% \\ \hline
		Music & 63,424 & 10.5\% \\ \hline
		Hotels \& Travel & 32,348 & 5.3\% \\ \hline
		Electronics & 31,853 & 5.3\% \\
		\hline
	\end{tabular}
	\caption{Product Categories of Reviews}
	\label{category_distro}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
Conclusion.


\bibliographystyle{ieeetr}
\bibliography{references}


\end{document}